<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">
  <title>About - Company Bootstrap Template</title>
  <meta name="description" content="">
  <meta name="keywords" content="">

  <!-- Favicons -->
  <link href="assets/img/favicon.png" rel="icon">
  <link href="assets/img/apple-touch-icon.png" rel="apple-touch-icon">

  <!-- Fonts -->
  <link href="https://fonts.googleapis.com" rel="preconnect">
  <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&family=Raleway:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&family=Nunito:ital,wght@0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&display=swap" rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="assets/vendor/aos/aos.css" rel="stylesheet">
  <link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
  <link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">

  <!-- Main CSS File -->
  <link href="assets/css/main.css" rel="stylesheet">

  <!-- =======================================================
  * Template Name: Company
  * Template URL: https://bootstrapmade.com/company-free-html-bootstrap-template/
  * Updated: Aug 07 2024 with Bootstrap v5.3.3
  * Author: BootstrapMade.com
  * License: https://bootstrapmade.com/license/
  ======================================================== -->
</head>

<body class="about-page">

   <header id="header" class="header d-flex align-items-center sticky-top">
    <div class="container position-relative d-flex align-items-center">

      <a href="index.html" class="logo d-flex align-items-center me-auto">
        <!-- Uncomment the line below if you also wish to use an image logo -->
        <img src="assets/img/logo.png" alt="">
        <h1 class="sitename">EMuLe</h1><span>.</span>
      </a>

      <nav id="navmenu" class="navmenu">
        <ul>
          <li><a href="index.html" class="active">Home</a></li>
          <li ><a href="team.html"><span>Team</span> <i class="active"></i></a>
            
          </li>
          <li><a href="blog.html">Publications</a></li>
        </ul>
        <i class="mobile-nav-toggle d-xl-none bi bi-list"></i>
      </nav>

    </div>
  </header>

  <main class="main">

    <!-- Page Title -->
    <div class="page-title accent-background">
      <div class="container d-lg-flex justify-content-between align-items-center">
        <h1 class="mb-2 mb-lg-0">About</h1>
        <nav class="breadcrumbs">
          <ol>
            <li><a href="index.html">Home</a></li>
            <li class="current">About</li>
          </ol>
        </nav>
      </div>
    </div><!-- End Page Title -->

    <!-- About Section -->
    <section id="about" class="about section">

      <div class="container">

        <div class="row position-relative">

          <div class="col-lg-9 about-img" data-aos="zoom-out" data-aos-delay="200"><img src="about_cut.png"></div>

          <div class="col-lg-7" data-aos="fade-up" data-aos-delay="100">
            <h2 class="inner-title">EMuLe: Enhancing Data and Model Efficiency in Multimodal Learning</h2>
            <div class="our-story">
              <h4>Project Duration: 01.09.2024 – 31.08.2027</h4>
			  <p>The rapid advancement of large neural networks, including Transformers and Large Language Models, has transformed the machine learning landscape. While these models exhibit extraordinary capabilities, their training demands copious amounts of data and computational resources. However, beneath the sheen of their astonishing accomplishments lies a significant challenge—data efficiency and resource optimization. This challenge becomes even more pronounced when these models are applied to multimodal data, where memory and computational efficiency are vital. Most existing studies in multimodal learning predominantly focus on image and text modalities. However, time series data, with its specific characteristics, presents another rich source of information in various applications, including sensor analysis in intelligent manufacturing, energy management, precision agriculture, and precision healthcare applications. The challenge of processing such multimodal data is magnified in scenarios where data resources are scarce, access to computational infrastructure is restricted, and cost and time factors are critical. Our goal is to propose data-efficient and model-efficient robust deep learning frameworks for multimodal data in applications with images, time series, and potentially additional modalities. Existing multimodal Transformers encounter two significant challenges in terms of efficiency: (1) data dependency and model parameter capacity: multimodal Transformers exhibit a data-hungry behavior attributed to their substantial model parameter capacity. This reliance on extensive training datasets poses a significant hurdle, limiting their applicability and scalability in real-world scenarios. Our motivation is to enhance data quality over data quantity, mitigating this data dependency and enabling these models to operate effectively even with limited training resources. (2) time and memory complexities in multimodal contexts: the inherent time and memory complexities of multimodal Transformers, growing quadratically with input sequence length due to self-attention, impose formidable constraints, particularly with jointly high-dimensional representations. We aim to comprehensively address these interdependent bottlenecks, seeking solutions that enhance the efficiency of training and inference processes, ultimately making multimodal Transformers more accessible and practical.
				</p>
              <p>Hence, our central research questions revolve around optimizing data-efficient training techniques to reduce the data required to train these large networks and ensure model efficiency by significantly reducing model size without compromising performance. The primary research questions are:</p>

			  <ul>
                <li><i class="bi bi-check-circle"></i> <span>How can we optimize data-efficient training techniques to minimize the data volume required for training Transformers and large neural networks while maintaining model performance?</span></li>
                <li><i class="bi bi-check-circle"></i> <span>How can we optimize data-efficient training techniques to minimize the data volume required for training Transformers and large neural networks while maintaining model performance?</span></li>
              </ul>

              
            </div>
          </div>

        </div>

      </div>

    </section><!-- /About Section -->


    <!-- Skills Section -->


    <!-- Clients Section -->

  </main>

  <footer id="footer" class="footer dark-background">

    <div class="container footer-top">
      <div class="row gy-4">
        <div class="col-lg-4 col-md-6 footer-about">
          <a href="index.html" class="logo d-flex align-items-center">
            <span class="sitename">Contact us:</span>
          </a>
          <div class="footer-contact pt-3">
            <p></p>
            <p>Et-Cetera-Building Karl-Wiechert-Allee 3, 30625 Hannover</p>
            <p class="mt-3"><strong>Postal address:</strong> <span> Hannover Medical School PLRI</span></p>
            <p><strong>Email:</strong> <span>Dr. rer. nat. Zahra Ahmadi, zahra.ahmadi@plri.de</span></p>
          </div>
        </div>
      </div>
    </div>

    <div class="container copyright text-center mt-4">
      <p>© <span>Copyright</span> <strong class="px-1 sitename">EMuLe</strong> <span>All Rights Reserved</span></p>
      <div class="credits">
        <!-- All the links in the footer should remain intact. -->
        <!-- You can delete the links only if you've purchased the pro version. -->
        <!-- Licensing information: https://bootstrapmade.com/license/ -->
        <!-- Purchase the pro version with working PHP/AJAX contact form: [buy-url] -->
        Designed by <a href="https://bootstrapmade.com/">BootstrapMade</a> Distributed by <a href=“https://themewagon.com>ThemeWagon
      </div>
    </div>

  </footer>

  <!-- Scroll Top -->
  <a href="#" id="scroll-top" class="scroll-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

  <!-- Preloader -->
  <div id="preloader"></div>

  <!-- Vendor JS Files -->
  <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="assets/vendor/php-email-form/validate.js"></script>
  <script src="assets/vendor/aos/aos.js"></script>
  <script src="assets/vendor/glightbox/js/glightbox.min.js"></script>
  <script src="assets/vendor/imagesloaded/imagesloaded.pkgd.min.js"></script>
  <script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
  <script src="assets/vendor/waypoints/noframework.waypoints.js"></script>
  <script src="assets/vendor/swiper/swiper-bundle.min.js"></script>

  <!-- Main JS File -->
  <script src="assets/js/main.js"></script>

</body>

</html>